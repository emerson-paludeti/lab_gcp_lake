# üöÄ Projeto: Data Crypto-Pipeline Enterprise

**Objetivo:** Capturar dados de mercado (API), garantir sua persist√™ncia (Data Lake) e realizar o processamento anal√≠tico (Data Warehouse).

----------

## üèóÔ∏è 1. Arquitetura do Ecossistema

1.  **Docker:** Cria o ambiente onde tudo funciona, isolado do seu computador.
    
2.  **Airflow:** Decide que horas o processo come√ßa a ser executado e o que fazer se algo quebrar.
    
3.  **Python:** Busca o dado e na API e faz a ingest√£o.
    
4.  **Google Cloud Storage:** Data Lake.
    
5.  **BigQuery:** Transforma o dado bruto em informa√ß√£o (Data Warehouse).
    

----------

## ‚òÅÔ∏è 2. Configura√ß√£o Detalhada: Google Cloud Platform (GCP)

### 2.1. O Projeto e Faturamento

-   **Nome do Projeto:** `lab-dados-gcp` (√â o identificador √∫nico de todos os recursos).
    
-   **Conta de Faturamento (Billing):** No GCP, nada funciona sem estar vinculado a uma conta de faturamento (mesmo no n√≠vel gratuito). O projeto foi associado √† sua conta de faturamento padr√£o para liberar o uso do BigQuery e Storage.
    

### 2.2. Google Cloud Storage (O Data Lake)

-   **Nome do Bucket:** `lab-dados-gcp-raw`
    
-   **Fun√ß√£o:** Funciona como um HD na nuvem. Aqui salvamos os arquivos `.json` puros, exatamente como vieram da API.
    
-   **Configura√ß√£o de Classe:** _Standard_ (Para acesso frequente) e Regi√£o _US_ (Mais barata e pr√≥xima dos servidores do BigQuery).
    

### 2.3. Google BigQuery (O Data Warehouse)

-   **Dataset:** `crypto_analytics` (√â a "pasta" que cont√©m as tabelas).
    
-   **Tabelas:**
    
    1.  **`BRZ_assets` (Bronze):** Dados crus. Se a API mandou 100 moedas, as 100 est√£o aqui com todos os erros e formatos estranhos.
        
    2.  **`SLV_assets` (Silver):** Dados limpos. Aqui o que era texto vira n√∫mero decimal e o que era bagun√ßado √© organizado.
        
    3.  **`GLD_market_summary` (Gold):** O produto final. Cont√©m apenas o resumo (m√©dias e categorias), pronto para a cria√ß√£o das vis√µes (relat√≥rios).
        ----------

## üìÇ 3. Guia de Arquivos: O que cada um faz?

Sua pasta `airflow-docker` √© o cora√ß√£o do projeto. Aqui est√° o papel de cada integrante:

**Arquivo**

**Fun√ß√£o Explicada**

**`docker-compose.yaml`**

√â o manual de instru√ß√µes do Docker. Ele diz: "Crie um servidor para o Airflow, um banco de dados para ele e conecte tudo".

**`Dockerfile`**

√â a receita de bolo da imagem. Ele pega o Airflow padr√£o e "instala" as ferramentas do Google dentro dele.

**`requirements.txt`**

Uma lista de compras. Cont√©m o nome das bibliotecas Python (como `google-cloud-bigquery`) que o script precisa para funcionar.

**`credentials.json`**

√â o seu crach√° de acesso. Sem esse arquivo, o Google barra a entrada do seu c√≥digo na nuvem.

**`pipeline_oficial.py`**

O c√≥digo principal. Ele faz a extra√ß√£o da API, o upload para o Storage e as consultas SQL no BigQuery.

**`agendador_crypto.py`**

O "despertador". Ele diz ao Airflow: "Todo dia, √†s 09:00, execute o arquivo `pipeline_oficial.py`".

**`.airflowignore`**

Um aviso de "n√£o entre". Diz ao Airflow para n√£o tentar agendar o script de execu√ß√£o, apenas o despertador.

----------

## üõ†Ô∏è 4. O Passo a Passo da Execu√ß√£o (O que aconteceu por tr√°s?)

1.  **O Comando `docker compose build`:** O Docker leu o `Dockerfile` e criou uma m√°quina virtual com Python e todas as ferramentas do Google instaladas.
    
2.  **O Comando `docker compose up -d`:** Ligou os servidores. O Airflow l√™ a pasta `dags`.
    
3.  **A Autentica√ß√£o:** O script Python l√™ o `credentials.json`. Ele usou esse arquivo para dizer ao Google: "Eu sou o administrador do projeto `lab-dados-gcp`, deixe-me entrar".
    
4.  **O Fluxo de Dados:**
    
    -   O Python buscou os dados na API CoinCap.
        
    -   Salvou um arquivo no Storage (Bucket).
        
    -   Mandou um comando para o BigQuery: "Pegue aquele arquivo que acabei de salvar no Storage e coloque na tabela Bronze".
        
    -   Mandou outro comando: "Agora limpe a Bronze e salve na Silver, e depois resuma na Gold".
        

----------

## üìä 5. Estrutura de Dados nas Tabelas

### Tabela Silver (`SLV_assets`)

-   `id`: Nome da moeda (ex: bitcoin).
    
-   `symbol`: Sigla (ex: BTC).
    
-   `priceUsd`: Pre√ßo convertido para n√∫mero (FLOAT64) para permitir c√°lculos.
    
-   `data_carga`: Hor√°rio em que o dado entrou no sistema.
    

### Tabela Gold (`GLD_market_summary`)

-   `categoria_mercado`: Classifica√ß√£o autom√°tica ("Alto Valor" para moedas caras).
    
-   `qtd_ativos`: Quantas moedas ca√≠ram naquela categoria.
    
-   `preco_medio_usd`: A m√©dia de pre√ßo daquele grupo.
    

----------

## üí° 6. Decis√µes de Projeto:

-   **Por que n√£o salvar direto no banco?** Se o banco de dados falhar, perdemos o dado. Salvando no Storage primeiro (Data Lake), temos um backup eterno do dado bruto.
    
-   **Por que o Airflow?** Se a API cair √†s 3 da manh√£, o Airflow vai tentar de novo sozinho. Voc√™ n√£o precisa acordar para consertar.
    
-   **Por que Docker?** Se voc√™ trocarmos a infra local, basta instalar o Docker e rodar um comando. Tudo funcionar√° igual, sem precisar configurar o Windows novamente.